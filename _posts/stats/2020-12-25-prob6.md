---
layout: post
mathjax: true
title: "Probability - Part 6 "
read: 15
secondary: prob&stat
date: 2020-12-25
---
## Part A - Probability basics in terms of discrete random variables

### 1. Display of probability mass distribution (p.m.f)

Since f(x) is a function, p.m.f can be presented in a tabular form; or using graph; or using formula [See various discrete distributions].

***Example 1:*** X = number of siblings of DSU students. So, X = {0,1,2,3...}. X is countably infinite number of possible values, X is a discrete random variable with a probability mass function. Find P(X = a specific value) = f(x). 

![](/sources/prob5-1b.png){:height="40%" width="40%"}

***Example 2:*** Random variable X = sum of two dice. Because X takes integers, it is discrete.

![](/sources/prob5-1.png){:height="50%" width="50%"}

We can draw a picture of the probability distribution of discrete random variable X by using relative frequency histogram. Depends on problems, probability distribution of discrete random variable has different shapes

![](/sources/prob5-3.png){:height="50%" width="50%"}

### 2. Find probability - use p.m.f

Probability mass function of a discrete random variable is the function f(x) = P(X = x). "Mass" means sum, so we can compute total mass as total sum of area (means probability)

![](/sources/prob6-2c.png){:height="40%" width="40%"}

![](/sources/prob6-3c.png){:height="30%" width="30%"}

Note: If the density varies continuously, we can find total mass using integration like probability density function in [Part 8] 

![](/sources/prob6-4c.png){:height="30%" width="30%"}

***Example 2 - continued***

P(at least 7) = P(7 <= X <= 12) = P(X=7) + P(X=8) + P(X=9) + P(X=10) + P(X=11) + P(12) = 6/36 + 5/36 + 4/36 + 3/36 + 2/36 + 1/36

P(less than 7) = P(X=2) + ...+P(X=6)

P(at most 10) = P(2 <= X <= 10) = 1 - (P(X=11) + P(X=12))

***Example 3***

![](/sources/prob5-2b.png){:height="70%" width="70%"}

***Example 4:***

![](/sources/prob5-3b.png){:height="60%" width="60%"}

### 3. Find probability - use cumulative distribution function (c.d.f)

+ Cumulative distribution function of a random variable X is fuction F(a) = P(X <= a)

+ F(a) is called the cumulative distribution function because F(a) gives the total probability that accumulates by ading up the probabilites p(x) as a runs from negative infinity to a. 

+ ***The c.d.f graph for a discrete random variable is always a step function (but c.d.f for a continuous random variable is always a continuous function)***

***Example 1***

![](/sources/prob6-5c.png){:height="60%" width="60%"}

=> P(2 < X < 3) = take all the probability of X from 0 through X < 3 substract all the probability of X from 0 through X < 2 = 7/8 - 4/8

***Example 2***: X = sum of 2 dice. (Here, sum 7 has the highest probability because it has the most possible values). 

![](/sources/prob6-6c.png){:height="60%" width="60%"}

### 4. Find probability - Determine p.m.f given c.d.f

Given c.d.f F(x), you can find the probability mass function (p.m.f) for any discrete random variable using c.d.f in this manner. This step function is for ***all discrete random variables***. 

![](/sources/prob5-7.png){:height="40%" width="40%"}

At X = 0, cdf jumps from 0 to 0.3 => P(X = 0) = 0.3. 

Next jump occurs when X = 1 and cdf jumps from 0.3 to 0.65m => P(X = 1) = 0.65 - 0.3 = 0.35

Next jump occurs at X = 2 where cdf jumps from 0.65 to 0.8 => P(X = 2) = 0.8 - 0.65 = 0.15

### 5. Mean (Expected Value), Variance of a Discrete Random Variable

**a. Mean (Expected value)**: 

+ Expected value is the long-term average value or mean after repeating an experiment theoretically infinite number of times. Example, if you only toss a fair coin 10 times, you might have 9 heads (90%). But if you toss a coins thousands of times, you will have around 50% heads and 50% tail because probability does not describe the short-term results of an experiment.
  
+  Karl Pearson once tossed a fair coin 24,000 times. He recorded the results of each toss and finally obtaining heads 12,012 times. In his experiment, Pearson illustrated the **Law of Large Numbers:** as the number of trials in a probability experiment increaes, the theoretical probability and the relative frequency get closer and closer together. Here, P(head) = Relative frequency(head) = 50%

+ Unlike the sample mean of a group of observations, which gives each observation equal weight, the mean of a discrete random variable weights each outcome x according to its probability p. So, **$\mu_x = \sum{x_i}{p_i}$**. 

![](/sources/prob6-5b.png){:height="20%" width="20%"}

***Example 1:*** Discrete random variable X = number of refrigerators sold

![](/sources/prob5-8.png){:height="40%" width="40%"}

E(X) = 0 * 0.3 + 1 * 0.35 + 2 * 0.15 + 3 * 0.1 + 4 * 0.05 + 5 * 0.03 + 6 * 0.02 = 1.42

=> Because more weight around 1 or 2, E(X) is approximately in around [1,2]. Over the long term, this salesperson should expect to sell 1.42 refrigerators per day on average, based on the data collected. In another word, over hundreds of days selling, you will come closer to the truth 1.42. That is why probability formulas are so helpful. 

***Example 2:*** Discrete random variable X = number of siblings of DSU students.

![](/sources/prob5-4b.png){:height="60%" width="60%"}

***Example 3:*** An individual plays a gambling game where it is possible to lose 1 dollar, break even, win 3 dollar, and win 5 dollar each time she plays. The probability distribution for each outcome is provided by the following table

![](/sources/prob5-6b.png){:height="30%" width="30%"}

The mean outcome for this game = -1 * 0.3 + 0 * 0.4 + 3 * 0.2 + 10 * 0.1 = 0.8

In the long run, the player can expect to win about 80 cents playing this game.

***Example 4:*** Imagine rolling dice 6000 times. You would expect to roll about 1000 ones, 1000 twos, and so on: about 1000 occurences of each possible outcome. Then, the average value of the outcomes obtained is:

![](/sources/prob6-1c.png){:height="40%" width="40%"}

**b. Variance**

![](/sources/prob5-9.png){:height="30%" width="30%"}

Shortcut formula: Variance = $E(X^2) - [E(X)]^2$

***Example 3 - continued:***

$[E(X)]^2 = 1.42^2$

$E(X^2) = 0^2 * 0.3 + 1^2 * 0.35 + 2^2 * 0.15 + 3^2 * 0.1 + 4^2 * 0.05 + 5^2 * 0.03 + 6^2 * 0.02 = 4.12$ 

=> Variance = 4.12 - 2.02 = 2.10

**Please read** [Part 7](https://lytranp.github.io/notes/prob7)
---------------------
**References**

Yale university, Statistics, http://www.stat.yale.edu/Courses/1997-98/101/