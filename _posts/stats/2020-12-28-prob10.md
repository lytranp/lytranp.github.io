---
layout: post
mathjax: true
title: "Probability - Part 10 "
read: 15
secondary: prob&stat
date: 2020-12-28
---
### Table of Contents
[1. Sampling distribution - Definition ](#samplingdistribution)

[2. Sampling distribution - Relationship with population distribution](#samplingdistribution2)

[3. Central limit theorem (CLT)](#centrallimittheorem)
  
[4. Central limit theorem - known population mean E(Y) and known standard deviation SD(Y)](#centrallimittheorem2)
  
[5. Central limit theorem - known population mean E(X) and unknown standard deviation SD(X)](#centrallimittheorem3)
                        -------------------------------*****--------------------------------------

The probability of normal distribution of [Part 9](https://lytranp.github.io/notes/prob9) shows that we had to make some restrictive assumptions (normality, known population mean, known population variance) in order to make statistical inferences. 

For example, if X (e.g. sale price) represents the population values and is normally distributed with mean (or expected value) E(Price) = 280 and SD(Price) = 50, we can find the probability to select randomly a house with sale price higher than 380 by converting to z-value.

![](/sources/prob10-1.png){:height="50%" width="50%"}

Our goal is to draw conclusions using fewer assumptions. 

### 1. Sampling distribution - Definition <a class="anchor" id="samplingdistribution"></a>

+ **Population parameter**: are population summary measures, represent the true values for the population and can not often be found exactly. E.g: population total T, population average $\mu$, and population proportion p.

+ **Sample statistics**: are summary measures from sample (e.g, total t, sample average $\bar{X}$, or sample proportion $\hat{p}$). We use them to estimate or predict numbers from population parameters.

=> To do this, we take random samples from the population over and over again, each time calculating a sample static (e.g, sample mean). Next, we display the histogram of these sample means: called the ***sampling distribution*** (sampling distribution of a statistic is the distribution of a statistic under repeated samples)

Note: Standard deviations is the variability of individual observations in a sample. Standard errors is variability of sample means.

![](/sources/prob10-6.png){:height="60%" width="60%"}

=> ***However, it is not actually necessary to take random samples over and over again. We can use results from probability theory to see what the sampling distributions is. So, all we need to do is take a single random sample, calculate a static, and we will know the theoretical sampling distribution of that statistic. For example, we will know what the statistic should if repeating samples, and how much the statistic should vary if repeating samples.***

### 2. Sampling distribution - Relationship with population distribution <a class="anchor" id="samplingdistribution2"></a>

Please see graph about the relationship between population distribution and sampling distribution [Here](https://gallery.shinyapps.io/CLT_mean/)

+ If population distribution is normal, the sampling distribution will be likely normal regardless of sample size.

+ If population distribution is right skewed or left skewed, and sample size is large enough, the sampling distribution will become normal.

+ As the sample size increases, we would have more consisten sample means, hence variability among the sample means would be lower which results in a lower standard error and the distribution is narrower. That is the reason why while mean of sampling distribution is approximately equal to the population mean, the standard error is approximately equal to SD of the populuation divided by square root of sample size = SD(Y)/$\sqrt{n}$

### 3. Central limit theorem (CLT) <a class="anchor" id="centrallimittheorem"></a>

![](/sources/prob10-7.png){:height="60%" width="60%"}

We do not often have population standardar deviation, so we use **"s" - the standard sample deviation of one sample we have to estimate the standard error**. Although we mentioned earlier about many samples, we only take one sample when running a study. The theorem holds true to a single random sample if either the population distribution is normal, or sample size is large (n > 30) when the population distribution is skewed. 

**Conditions for using CLT**

+ Samples observations must be independent. This is difficult to verify but it is more likely if we use random sampling 

+ If sampling without replace, the sample size n < 10% of population. The reason is because we love large samples, but we do not want our sample size to be very large because if too many, sampled individuals could be dependent each other. 

+ If sample is taken from normal population distribution, regardless of sample size. If the population distribution is skewed, sample size has to be large (> 30). Otherwise, it will mimic the skewed population distribution. When we increase the sample size, the standard error decreases and the distribution starts to condense around the mean and look symmetric. The more skewed population distribution is, the larger sample size we need for CLT to apply. It is difficult to verify population distribution, so we make plot of the sample data and assume that it mirrors population distribution. 

### 4. Central limit theorem - known population mean E(Y) and known standard deviation SD(Y) <a class="anchor" id="centrallimittheorem2"></a>

***Example***: Suppose my iPod has 3,000 songs. The histogram below shows the distribution of lengths of these songs. We also know that for this iPod, the mean length of all songs is 3.45 minutes and the standard deviation is 1.63 minutes. 

*P(randomly selected song lasts more than 5 minutes) ?*

![](/sources/prob10-8.png){:height="50%" width="50%"}

If I drive 6 hours and make a random playlist of 100 songs in iPod. P(my playlist lasts the entire drive) ? 

![](/sources/prob10-9.png){:height="50%" width="50%"}

### 5. Central limit theorem - known population mean E(X) and unknown standard deviation SD(X) <a class="anchor" id="centrallimittheorem3"></a>

One major drawback to the normal version of the central limit theorem is that to use it, we have to assume that we know the value of the population SD(Y). To solve this problem, we use Student's t-distribution in which replacing SD(Y) with repeated sample standard deviations, $S_Y$.

t-distributions are little more spread out than the normal distribution. This "extra variability" is controlled by degrees of freedom. Because we use an estimate of the population standard deviation, we are not sure about the population and if our sample standard deviation is a good estimate of the population standard deviation. So, it makes sense that the degrees of freedom is lower for smaller sample sizes. 

![](/sources/prob10-5.png){:height="30%" width="30%"}

***Example***: In this example, we are not thinking about the specific sample mean we calculate from actual sample of 30 sale price $\bar{X} = 278$. Rather, we are imagining that we have a list of sample means (each sample has size = 30) from a population distribution with mean E(X) = 280. Then, we calculate mean and standard deviation for each sample. So, we mean of sample means $M_Y$ and standard deviation of these repeated samples $S_Y$ .

=> t = $\frac{M_Y - E(Y)}{\frac{S_Y}{\sqrt{n}}}$

**Please read** [Probability - Part 11](https://lytranp.github.io/notes/prob11)

-------------------------------------------------
**References**

PennState, Eberly College of Science, https://online.stat.psu.edu/stat462/node/251/
