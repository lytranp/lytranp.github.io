---
layout: post
mathjax: true
title: "Probability - Part 11 "
read: 15
secondary: prob&stat
date: 2020-12-29
---
In the [Part 10](https://lytranp.github.io/notes/prob10), we build up intuition about sampling distributions. So far, we have focused on the sampling distribution of sample means $M_Y$, but what we would really like to do is infer what the observed sample mean, $m_Y$, tells us about the population mean E(Y)

To make statistical inferences about the population (e.g, population mean), we have 2 types:

+ Estimate population parameters

+ Test hypotheses about parameters

### 1. Estimate population parameters - use Interval Estimation

An important assumption about point estimates is that they are unbiased (meaning that it does overestimate or underestimate, it provides a good estimate) because the sampling distribution of the estimate is centered at the true population parameter it estimates. The sample mean is an example of an unbiased point estimate for population mean. We want to know how reliable this estimate is. So, we need to calculate an **interval estimate** or **confidence interval** for the population mean E(Y).

+ Confidence interval (CI): is a range of numbers which most likely contain actual population value.

+ Confidence level (CL): is probability that interval actually contains the population value. We have to identify CL we want first because CL will decide CI.

+ To use CI, we have to assume that: a random sample we choose has sample size, example 50 < 10% of population size to make sure individuals in the sample are independent of another. Besides, if random sample size n = 50 > 30, we can assume that the sampling distribution of point estimate (sample mean) from samples of size 50 will be nearly normal. 

+ Margin of error includes z-score (critical value at XX% confidence interval) and standard error 

![](/sources/prob11-13.png){:height="40%" width="40%"}

![](/sources/prob11-12.png){:height="40%" width="40%"}

![](/sources/prob11-14.png){:height="40%" width="40%"}

#### When to use z-distribution and t-distribution ?

![](/sources/prob11-3.png){:height="70%" width="70%"}

![](/sources/prob11-4.png){:height="70%" width="70%"}

Please see more [Examples](https://www.mathbootcamps.com/calculating-confidence-intervals-for-the-mean/)

#### Interpretation about interval estimation

+ We **cannot** say: "probability that population mean is in our interval is 95%" because we do not know whether 95% confidence interval for our particular sample contains the population mean. Additionally, the the population mean value is fixed.

+ Loosely speaking, 95% of the time, when we calculate a confidence interval in this way, the true mean will be between 2 values and 5% of the time, it will not. Because the true mean (population mean) is an unknown value, we do not know if we are in the 5% or 95%. But 95% is pretty good. So, we can say that: "we are 95% confidence that the mean of house sale price in the market is between 258,000 and 299,000". 

+ Strictly speaking, if we were to take a large number of random samples of size 30 from our population of sale prices and calculate 95% confidence interval for each, then 95% of those confidence intervals would contains the population mean. Specifically, suppose we took many samples and built a confidence interval from each sample using the equation: "point estimate +/- 1.96 * SE", then about 95% of those intervals would contain the true population mean $\mu$
  
+ In this below figure, the vertical line represents the true population mean which we rarely know, and each horizontal line is an iterval calculated based on a different random sample. There are 25 total intervals plotted, and 24 of them contain the true population mean, and 1 (red line) does not. This means that 24/25 = 96% of those intervals contain population parameter. For example, in a survey 1,154 US residents in 2010, 95% CI for the average number of hous reading is 1.5 to 1.8 hours. Because we know exactly what the sample mean is, we can say that we are 100% confident that US residents *in this sample* spend on average 1.5 to 1.8 hours on reading. However, CI is not about the sample mean, not individuals in the population (so, it is false to say that 95% of US residents spend 1.5 to 1.8 hours reading), instead CI about the population mean. So, the statement means that 95% of random samples of 1,154 US residents will yield CI that contain the true average number of hours US residents spend reading.

+ When we increase confidence level, we can capture population parameter more accurate because the width of interval will increase. Unfortunately, it comes at cost. CL increases, width of interval increases, the accuracy increases, but precision goes down. So, we do not always use 99% confidence level because wider interval makes conclusion to become useless. For example, "I am 99% confident that you will score between 10 and 100 on your next exam". This prediction seems not useful because the interval is too wide. 

+ In order to get higher precision and higher accuracy, we need to increase our sample size. This will shrink our standard error. Therefore, we can still remain at high CL while not necessarily increase the width of confidence interval

![](/sources/prob11-10.png){:height="40%" width="40%"}

![](/sources/prob11-11.png){:height="40%" width="40%"}

### 2. Estimate population parameters - use Hypothesis Testing

Hypothesis testing is a procedure in inferential statistics. The sample data must provide sufficient evidence to reject the null hypothesis.

**Hypothesis testing using simulation**

***Example***: Is there statistical difference about the promotion rate of male and female ?

$H_0$: Promotion and gender are independent (there is nothing going on: no gender discrimination) and observed difference in proportions is simply due to chance

$H_a$: Promotion and gender are dependent (there is something going on: gender discrimination) and observed difference in proportions in not due to chance

Solution using simulation: At each simulation, we calculated the difference between proportion of promotions between male and female. Each one of these dots in the dot plot represents one simulated difference between promotion proportions of male and female. Because the difference at each simulation was distributed quite equally, and 30% does not seem like a usual outcome. *Note: "more extreme" means in the data, male promotion is 30% more or even higher than female promotion*. 

![](/sources/prob11-16.png){:height="40%" width="40%"}

![](/sources/prob11-15.png){:height="40%" width="40%"}

**Hypothesis testing using theoretical methods (rely on CLT)**

***Example***: a real estate agent claims that mean sale price in this market is 255,000 dollars. To check this information, you select a sample of size 30. After calculating from this sample, we have $m_Y$ = 278,6033 dollars and $s_Y$ = 53.8656. So, does information from our sample support the agent's claim or does it favor an alternative claim ? 

**Procedure to conduct a hypothesis test**

+ Null hypothesis $H_0$: parameter = some hypothesized value. H0 is equivalent to a presumption of innocence in a trial before any evidence has been presented

+ Alternative hypothesis $H_a$: represents for research question. It can be a lower-tail form: E(Y) < 255; or upper-tail form: E(Y) > 255; or 2-tail form: E(Y) != 255

+ Conduct a hypothesis test under the assumption that $H_0$ is true, either via simulation of theoretical methods. Calculate a test statistic, this test statistic will have a particular probability distribution (e.g, t-distribution)

![](/sources/prob11-5.png){:height="80%" width="80%"}

+ To decide how far from zero a test statistic would have to be before we reject $H_0$ and favor $H_a$, we must make a decision about how much evidence we will require before rejecting $H_0$. There is always a chance that we might mistakenly reject $H_0$ when it is actually true. This chance is called the **significant level** that dictates the critical value for the test. 

+ If the test result suggest that the data do not provide convincing evidence for $H_a$, we stick with $H_0$ and vice versa. 

![](/sources/prob11-19.png){:height="70%" width="70%"}

**a. Hypothesis testing using theoretical methods: test statistic and critical value**

Basically, firstly we convert observed statistics (sample mean) to z-score on standard normal distribution. After that, we see critical value at XX% confidence interval. Finally, compare the position of z-score and critical value. Here, test statistic = 2.40 while critical value for 95% CI = 1.699. Because test statistic is far behind critical value, 2.40 falls in rejection region and we reject $H_0$ 

![](/sources/prob11-6.png){:height="90%" width="90%"}

![](/sources/prob11-7.png){:height="40%" width="40%"}

**b. Hypothesis testing using theoretical methods: test statistic and p-value**

Another way to conduct a hypothesis test is use test statistic to calculate p-value

**p-value** = P(observed or more extreme outcome $\vert$ $H_0$ true)

If p-value < significant level $\alpha$ (0.05), this suggests that it would be very unlikely to observe extreme data if $H_0$ true. So,reject $H_0$. 

If p-value > significant level $\alpha$ (0.05), it would be likely to observe extreme data even if $H_0$ was true. So, do not reject $H_0$. 

**Interpretation:** 

+ p-value = P(obtaining a random sample of 50 students that have score mean >= 3.2, if in fact students truly had score mean = 3) = 0.209. 

+ It means that if in fact students had average score = 3, there is a 21% chance that a random sample of 50 students would yield a sample of 3.2 or higher. Because this probability is pretty high, we think that a sample mean of 3.2 or even more is likely to happen simply by chance. So, even though we observed a sample mean slightly above 3, there is no convincing evidence that students have more than 3 score on average. The difference between $H_0 = 3$ and the observed sample mean of 3.2 is ***due to chance or sampling variability.***

![](/sources/prob11-17.png){:height="40%" width="40%"}

![](/sources/prob11-18.png){:height="40%" width="40%"}

Please see more explaination [p-value](https://s4be.cochrane.org/blog/2016/03/21/p-value-in-plain-english-2/#:~:text=In%20academic%20literature%2C%20the%20p,the%20null%20hypothesis%20were%20true)

### 3. Hypothesis test errors

**a. Hypothesis testing has 2 types of errors: Type I and Type II**

![](/sources/prob11-8.png){:height="60%" width="60%"}

+ Type I erros is when we reject $H_0$ although it is really true. It is equivalent to false alarm: fire alarm rings when there is no fire (False Positive). Even though we do not know for sure which studies have False Positive results, we do know their rate of occurrence. The rate of occurrence for Type I errors = significance level of the hypothesis test (also known as alpha $\alpha$)

+ Type II error rate (also known as beta $\beta$) is when we do not reject $H_0$ although it is really false. 1-$\beta$ = the statistical power = the probability of correctly detecting $H_a$ (effect exists). The shaded region on $H_a$ distribution represents the Type II error rate. The rest of alternative distribution represents the probability of correctly detecting an effect: called ***power***.
  
+ If we lower significance level, we reduce the chance of type I error occuring. Unfortunately, there is a tradeoff between Type I and Type II error. As you reduce the chance for False Positive, you increases the opportunity for False Negative. 

+ Moving the critical value line changes the significance level. Moving the line to the left, we are increasing signifciance level. This adjustment increases the Type I error rate, while reducing the Type II error rate.Moving the line to the right reduces the significance level, which decreases the Type I error rate, but increases the Type II error rate. 

![](/sources/prob11-9.png){:height="40%" width="40%"}

**b. Which error is the worst error to make ?**

In criminal trial:

+ $H_0$: Defendent is innocent

+ $H_a$: Defendent is guilty

Type I error: Although defendent is innocent, the judge declare the defendant guilty.

Type II error: Although defendent is actually guilty, the judge declare the defendant innocent.

The decision of choosing the worst error is subjective. However a quote from William Blackstone "better that 10 guilty persons escape than that one innocent suffer"

+ P(Type I error $\vert$ $H_0$ true) = $\alpha$. E.g 5% signficance level: There is around 5% chance of making a type I error if $H_0$ is true. Increasing $\alpha$ increases type I error. If type I error is dangerous or especially costly, choose a small significance level. Our goal is that we can have strong evidence favoring $H_a$ because the probability of rejecting $H_0$ wrongly is too small.

+ If type II error is more dangerous or much more costly, choose a higher significance level. 

![](/sources/prob11-20.png){:height="40%" width="40%"}

-----------------
**References**

https://statisticsbyjim.com/hypothesis-testing/types-errors-hypothesis-testing/

https://www.coursera.org/learn/inferential-statistics-intro/home/welcome
