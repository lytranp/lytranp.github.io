---
layout: post
mathjax: true
title: "Classification - ROC/AUC"
read: 15
secondary: datamining
date: 2020-12-06
---

### 1. Logistic/Sigmoid function

Because classification problem only have target categorical (or binary) variables, e.g predict obese (1) or not obese (0), we can not draw a function like linear regression. That is the reason we have to choose another function that its shape can represent data better. In classification problem, most are Logistic/sigmoid function.

 ![](/sources/roc2.png){:height="40%" width="40%"}

 **Note**: Purple is actual positive; and black is actual negative in our case study today. 

Original data only shows weight and we have to predict if the person is not obese (0) or obese (1). Some people has very low weight, then we are quite confident that they are not obese. Similarly, some people has very high weight, and we are quite sure that they are obese. How is about the other people with weight not too low but also not too high ? Even our prediction still might be wrong for those who we are quite confident. There is no way to be confident 100%. Consequently, we only can say ***the probability*** of obesity. Each person will have an obesity probability based on their weight. 

Next, we need one way to convert these probabilities into classification: Obese or Not obese. It is the goal of problem. So, we need a threshold (cutoff) to classify. Assuming that we choose a threshold = 0.5. After that, we will compare prediction with actual value using **Confusion Matrix**


### 2. Confusion Matrix

![](/sources/roc3.png){:height="60%" width="60%"}

In our above graph, we have:

- 9 patients in total

- At cutoff = 0.5:
  
  + 4 patients are assigned "Obese" correctly: TP  => a = 4 

  + 1 patient are assigned "Obese" wrongly: FP => c = 1

  + 3 patients are assigned "Not Obese" correctly: TN => d = 3

  + 1 patient are assigned "Not Obese" wrongly: FN => b = 1

![](/sources/roc4.png){:height="80%" width="80%"}

Terminology about confusion matrix in healthcare:

| **TP** | **FP** |
|-----|-----|
Have disease & assign correctly | Predicted Positive (Disease) BUT it is false prediction because actually, they is Negative (No Disease) => No Disease BUT assign to Disease.

| **TN** | **FN** |
|-----|-----|
No disease & assign correctly | Predicted Negative (No Disease) BUT it is false prediction because actually, they is Positive (Disease) => Disease BUT assign to No Disease

### 3. Accuracy score 

Here is an example why accuracy score does not work well. If we guess randomly that all patients is Negative (Not disease), our prediction is still correct to (6/7) = 85%. That is because of imbalanced data

![](/sources/roc1.png){:height="30%" width="30%"}

### 4. Sensitivity / Recall / TPR

$$ \frac{TP}{TP + FN} $$

![](/sources/roc5.png){:height="40%" width="40%"}

**Interpretation**

Recall = $\frac{4}{4+1}$

Recall describes the number of positive cases the model detected correctly out of total actual positive cases. Actually, we have 5 obese patients, but the model only can detected 4. ***Our one actual positive case was wrongly assigned to negative (FN)***. So, we have to add TP with FN to have total actual positive cases. 

=> To find FNR

 $$
 1 - \frac{TP}{TP + FN}
 $$

### 5. Specificity

$$ \frac{TN}{TN + FP} $$

![](/sources/roc6.png){:height="40%" width="40%"}

**Interpretation**

Specificity = $\frac{3}{3+1}$

Specificity describes the number of negative cases the model detected correctly out of total actual negative cases. Actually, we have 4 NOT obese patients, but the model only can detected 3. ***Our one actual negative case was wrongly assigned to positive (FP)***. So, we have to add TN with FP to have total actual negative cases. 

=> To find FPR (*False alarm rate*)

> 1 - $\frac{TN}{TN + FP}$ = $\frac{FP}{TN + FP}$


### 6. ROC, Trade-off relationship between TPR and FPR

At each different threshold, combination of {TP, TN, FP, FN} vary. Ideally, we would like to have a threshold that can correctly classify all. 

![](/sources/roc7.png){:height="40%" width="40%"}

***To avoid missing positive cases, we predict most of them are positive. Because we try to assign many positive cases as possible (TP increases), we also pick negative cases to label them as positive, leading to FP increases. This also remains true to negative cases. If we simply predict most of them are negative, then yes, the number of True Negative will increase, but at the same time, some of them that are positive picked wrongly, leading to FN increases***

**Conclusion**: There is always a trade-off between TPR and FPR. When the number of TP cases increases, the number of cases we wrongly assigned to Positive also increases (meaning FP increases). 

To clarify this trade-off, we will check TPR and FPR at different threshold values. 

![](/sources/roc8.png){:height="40%" width="40%"}

+ If threshold is 0.5, we did not predict correctly all positive cases and negative cases. So, FP and FN still exist.

![](/sources/roc10.png){:height="40%" width="40%"}


+ If we let threshold too lower, we can predict correctly all positive cases (TP increases), but at the same time all of 4 actual negative cases are assigned wrongly to positive. Consequently, TPR is 100% and FPR is also 100%. The blue diagonal line shows where TPR = FPR. If we choose this threshold to predict, it turns out that we are predicting 50 - 50.  

![](/sources/roc9.png){:height="40%" width="40%"} ![](/sources/roc11.png){:height="40%" width="40%"}

- If we let threshold higher (Purple line = 0.6), the number of positive cases are predicted correctly (TP) decreases to 3. Consequently, the number of cases are wrongly assigned to positive (FP) also decreases to 1. We will show the relationship between TP and FP on the graph. 

TPR = $\frac{3}{3 + 2}$ = 0.6 

FPR = $\frac{1}{1 + 3}$ = 0.25

![](/sources/roc12.png){:height="40%" width="40%"}


+ If we let threshold higher (Red line = 0.6), the number of negative cases are predicted correctly (TN) increases to 4. Consequently, the number of cases are wrongly assigned to negative (FN) also increases to 3. From this table, we will show the relationship between TP and FP on the graph. 
  
TPR = $\frac{2}{2 + 3}$ = 0.4 

FPR = $\frac{0}{0 + 4}$ = 0

![](/sources/roc13.png){:height="40%" width="40%"}

Finally, connecting all of intersections between TPR and FPR that we just computed at different thresholds, we will have ROC curve as the below

![](/sources/roc14.png){:height="40%" width="40%"}


### 7. Another example of ROC

This graph shows ROC that connects all of intersections between TPR and FPR at different thresholds

![](/sources/roc15.png){:height="40%" width="40%"}

- We often want high TPR and low FPR. Sadly, often TP increases, then FP also increases. And vice versa. 

- The graph shows point A > point B because at the same TPR, point A has lower FPR

- Depend on how many FP we are willing to accept, we can choose optimal cut off that leads to point A or C or D. E.g. For classifying harmful video (Positive), we accept many good videos are assigned wrongly rather than many bad videos are not warned because they are assigned to negative. Consequently, we need to increase FP, meaning that we should choose C instead of A. 



